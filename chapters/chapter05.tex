\chapter{Branch specific codon models\label{chap:dpp}}

\begin{remark}{Outline}
In this chapter
TODO
\end{remark}

\section{Introduction}

First computationally trackable models of codon substitution were independently proposed by \cite{Muse1994} and \cite{Goldman1994} and published in the same issue of Molecular Biology and Evolution (MBE).
For codon models the non-stop codon triplet $n_{1}n_{2}n_{3}$ is considered the smallest unit of evoluton.
There are $4^3$ possible triplets minus three stop-codons, resulting in a state space size of $61$ codons.
The standard assumption of independence is used, i.e. the substitutions at these three codon positions occur independently, thus only a single change per triplet can occur at a given time.
Selective pressure is the main force behind the molecular evolution.
Proteins are coded by a set of 20 amino acids, with each amino acid being coded by a codon triplet. 
Because there are 61 non-stop codons, inevitably some amino acids will be redundantly coded by more than 1 codon.
% Such a change 
A substitution which does not change the amino-acid is called a synonymous 
one,
% substitution 
as they most likely leave the protein function unchanged.
Conversly non-synonymous substitutions are more likely to affect the fitness of a particular organism.
Understanding and inferring the selective pressure is one of the central goals of molecular virology, see for example \citet{Bielejec2014a}.
That is why most of the codon substitution models are parametrized in terms of the rate of non-synonymous (denoted by convention $\beta$) and synonymous ($\alpha$ by convention) substitutions.
Their ratio $\omega=\beta / \alpha$ is a standard measure of the selective pressure \citep{ThePhylogeneticHandbook}, which is sometimes also denoted $\omega = dN/dS$.
Prevalence of synonymous substitutions over non-synonymous ones is leading to a \emph{purifying (negative)} selection, and corresponds to the ratio $\omega <1$.
If non-synonymous substitutions accumulate at a faster rate than the synonymous substitutions this is the \emph{positive selection}, improving the fitness of the particular organism.
$\omega\approx 1$ signifies neutral evolution.

There are several advantages in using codon models over nucleotide based substitution models.
Not all DNA positions evolve at the same rate, with non-synonymous substitutions occuring more frequently then synonymous substitutions.
Although this problem can be mitigated to some extent by using codon-positioned nucleotide substitution models, the fast evolving positions and the state space limited to 4 character alphabet still can lead to biased estimates over long evolutionary distances, as portrayed in the previous Chapter~\ref{chap:pibuss}.

% GY style models
The model proposed by \cite{Goldman1994} is characterized by a substitution rate matrix with following entries:

\begin{equation}
\footnotesize{
q_{ij}^{GY94}=\begin{cases}
\pi_{j} & \text{if \ensuremath{i\rightarrow j} is a synonymous transversion}\\
\kappa\cdot\pi_{j} & \text{if \ensuremath{i\rightarrow j} is a synonymous transition}\\
\omega\cdot\pi_{j} & \text{if \ensuremath{i\rightarrow j} is a non-synonymous transversion}\\
\omega\cdot\kappa\cdot\pi_{j} & \text{if \ensuremath{i\rightarrow j} is a non-synonymous transversion}\\
0 & \text{otherwise}
\end{cases},
} % END: size
\label{eq:gy94}
\end{equation}

\noindent
where parameter $\kappa$ denotes the transition/transversion rate ratio, parameter $\omega$ denotes the non-synonymous/synonymous
rate ratio and $\pi_j$ denotes the equilibrium frequency of codon triplet $j$.
Parameter $\kappa$ and $\pi_j$ can be though of as controlling the CTMC process at the DNA level, while $\omega$ parameter characterizes the selection on non-synonymous substitutions.
For the GY94 model the synonymous evolutionary rate is fixed to be 1, i.e. $\omega=\beta$.

Different flavours of the GY94 model differ in the composition of the equilibrium codon frequency parameter $\pi_{j}$.
One approach is to model the codon frequencies as each having the same long-time frequency of appearing. 
Such model is reffered to as the GY94-Fequal.

In the GY94-F$1\times4$ model codon frequencies are expected from the four nucleotide frequencies, with frequencies being pulled from all three codon positions, i.e. $\pi_{n_{1}}=\pi_{n_{3}}=\pi_{n_{3}}$ for all four nucleotides,.
Thi smodel leads to 3 free parameters that need to be estimated from the data.
If the frequencies are parametrized according to three sets of nucleotide frequencies for the three codon positions, resulting in nine free parameters, the model is called the GY94-F$3\times4$.
Finally in the GY94-F61 model every codon triplet has it's own frequncy parameter, with all parameters summing to one, resulting in 60 free parameter that need to be estimated.

% pluses positive and negative of these models :)
The GY94-F61 model has been suggested as important in letting the model explain the data without constricting assumptions.
\citet{Rodrigue2008} however argue that the model has no intepretation on the nucleotide level as well as confounds other effects. 
Same authors criticize the GY94-F$3\times4$ setup as superficial, pointing that the periodic pattern along the nucleotide sequence positions represents the coding structure of the sequence but not the processes that shape the volution.

% MG style models
The Markov generator matrix for MG94 style models \citep{Muse1994} is given by:

\begin{equation}
\footnotesize{
q_{ij}^{MG94}=\begin{cases}
\alpha\cdot\kappa\cdot\pi_{n}^{p} & i\rightarrow j\text{ is a synonymous transition from nucleotide \textit{m} to \ensuremath{n} at codon position \ensuremath{p}.}\\
\alpha\cdot\pi_{n}^{p} & i\rightarrow j\text{ is a synonymous transversion from nucleotide \ensuremath{m} to \ensuremath{n} at codon position \ensuremath{p}.}\\
\beta\cdot\kappa\cdot\pi_{n}^{p} & i\rightarrow j\text{ is a non-synonymous transition from nucleotide \ensuremath{m} to \ensuremath{n} at codon position \ensuremath{p}.}\\
\beta\cdot\pi_{n}^{p} & i\rightarrow j\text{ is a non-synonymous transversion from nucleotide \ensuremath{m} to \ensuremath{n} at codon position \ensuremath{p}.}\\
0 & \text{otherwise.}
\end{cases},
} % END: size
\label{eq:mg94}
\end{equation}


There are two main differences to be noted with respect to the GY94 style codon models.
First the model is parametrized in terms of both the non-synonymous ($\beta$) and synonymous ($\alpha$) rates, meaning that a change in the ratio may be due to alteration of the rate of non-synonymous or synonymous substitutions, or both.
Second of all the model estimates the frequencies of the target nucleotide $\pi_{n}^{p}$ rather than of the target codon triplet $\pi_{j}$, as in GY94 model.
As before the MG94 model can be used with frequencies specified as a single (MG94-F$1\times4$) or three (MG94-F$3\times4$) vectors with 4 dimensions.
For the remainder of this chapter we will focus our estimation on the MG94 parametrization, mostly because the parametrization in terms of both the synonymous and non-synonymous substitution rates offers interesting opportunities for disentangling
the patterns of correlation between
the two.


% first talk about across site variation
Molecular phylogenetic analysis using codon models are typically restricted to fitting a single model, with constant parametrization across data, i.e. sites of the alignment.
However there is no biological reason to assume that all the sites are under the same selection regime.
\cite{NY98} point towards the fact that for most real world data-sets for which we know that evolved under positive selection pressure only a handfull of sites actually contributed towards the non-synonymous to synonyous rates ratio >1.
Therefore applying constant model to the data, without accounting for across-data heterogeneity biases the estimates of $dN/dS$ ratio towards lower values.
In some corner cases this might lead to a failure in detecting positive selection even if it actually exists.  
In that respect \cite{NY98} propose two models in GY94 context, with fixed number of categories to which a particular site can belong.
Their \emph{neutral model} assumes a category for sites where the nonsynonymous mutations are neutral ($\omega=1$) and a category for sites which are conserved ($\omega=0$).
\emph{Positive selection model} adds an extra category of positively selected sites ($\omega>1$).
\cite{Goode2008} employ this setup and further extend it by allowing for a change in $\omega$ parameter value at some specified time between the most-recent common ancestor and the most recent sampling time.
These models, although undeniably a step in the right direction, still make strong assumptions, among which partitioning sites to fixed number of rate classes and estimating the rate for each class separately is probably the most restrictive one.
Ideally the model should let the data decide how many rate classes there are, and what the site-specific values are.
First modelling approach that comes to mind is to fit a different rate to each site in the alignment \citep{Nielsen1997}.
Although this approach might seem naive at first it bears a valuable property in that no explicit assumption in the distribution of rates across the sites is being made.
However other negative factors still persist, precluding such a class of methods from being generally applicable.
Main problems is that, particularly for long sequences, many parameters need to be estimated, magnifying the computational costs.
In case of shorter sequences such a method may overfit the data, leading to significant loss of statistical power and biased estimates.
Another approach treats site-specific rates as independent draws from some common distribution, e.g. gamma distribution \citep{Yang1993} or inverse Gaussian probability distribution \citep{Waddell1997}, sometimes with a proportion of sites allowed to remain invariable \citep{Gu1995}.
This approach allows to model the rate variation with relatively few parameters, as well as provides pooled estimates of the distribution parameters, which can in turned be compared across different data-sets.  
In many scenarios though a single parametric distribution proves too naive of an approach to capture complex patterns of rate evolution \citep{Pond2005a}.
\cite{Yang2000} proposed to model among-site substitution rate variation in codon models by a mixture of different distributions.
Although mathematically possible it proved computationally too expensive to fit mixtures of continuous distribution for the parameters of interest and the authors resort to fitting discrete approximations.
Although these constraints are being actively stretched \citep{Suchard2009,Ayres2012} averaging over a large array of distributions remains computationally intensive. 
\citet{Pond2005a} propose a simple, yet flexible model of rate variation with an integrative method to reliably estimate the fluctuating parameters.
This method proceeds by partitioning the rate distribution into countable number intervals, with each interval represented by some statistic.
Then a beta model is used to decide what intervals the underlying rate distribution is partitioned into and calculate corresponding rate class values.
This is perhaps the most flexible approach for evaluating site-specific rate variation to date, that provides a good fit to data with only a few extra parameters.

The molecular clock hypothesis \citep{Zuckerkandl1962} posits that the rate of evolution of any specified protein remains constant over time and over different lineages.
This hypothesis is compelling, mostly because of it's simplicity and because it allows to date bifurcation events on the tree of life.
There is however no biological motivation to assume constant pattern of the parameters shaping the substitution process across branches, it is rather a computational convenience.
In fact the evidence for heterogenous rates of evolution have been found and demonstrated in several studies \citep{EyreWalker1997,Andreasen2001} and the strickt molecular clock hypothesis is often rejected for real-world data sets.
In comparison to across-data variation relatively less work has been aimed at modelling across-lineage rate heterogeneity.

Unfortunately allowing for an uconstrained changes in branch substitution rates does not facilitate the main advantage of molecular clock hypothesis - the ability to date the speciation events on the tree.
% the rate and time along each branch can only be estimated as their product
This is due to the fact that the branch time is a product of the expected number of substitutions per site (rate, $r$) and the branch length in time units ($t$), causing the two to be confounded in phylogenetic analysis (see also Subsection \ref{sub:rates}).
These problems have lead to the proposal of so called "relaxed molecular clock" models, which allow for a changes in rate over time, but in a constrained manner, allowing for the divergence time estimation.
\citet{Drummond2006} propose several uncorrelated relaxed clock models in which the substitution rate can vary among the branches of the tree by being drawn independently and identically from some distribution.
This work has been further extended by \citet{Drummond2010} who developed a Random Local Clock (RLC) model, which does not rely on the number and location of rate changes being fixed.
Instead RLC model allows for constructing proposal which move both in the tree space and in the clock structure space at the same time, allowing for a joint reconstruction of the topology as well as the divergence time estimation. 
Their approach utilizes the fact that for most datasets there exists only a small number of rate changes and that subclades on a tree will share the same rate.
This allows them to efficiently sample from the clock rate space using Bayesian Stochastic Search Variable Selection (BSSVS, \citet{Lemey2009}) 

Here we propose an approach similar in spirit to the Random Local Clock models.
Although our method allows for modelling the branch rates, our main interest lies in modelling across-lineage heterogeneity of the parameters of codon substitution models, such as the synonymous and non-synonymous substitution rates.
Both the number and the clustering of the lineage specific parameters are treated as random, and are controlled by a Dirichlet Process Prior (DPP, \citet{Ferguson1973}).
A DPP is a "distribution over distributions" and induces clusterings with a varyng number of components that can grow or shrink depending on the data and a concentration parameter.
It is thus often used as a prior over possible clusterings where the exact number and population of distinct clusters is not known beforehand.
We require no structure to be enforced on the clustering of the branches, and allow the data to drive the model in a descriptive fashion. 
Our only requirement is the prior specification of the concentration parameter of DPP, controlling whether model preffers less clusters with many occupants or more less densely populated ones. 
By implementing our model as part of BEASTs Bayesian inference framework we allow for a hyperprior to be put on this parameter to average over all of its possible values. 
This approach avoid overparametrisation and computational overhead by achieving data-driven balance between simplicity and complexity of the model.
We use Markov chain Monte Carlo (MCMC) techniques, in which a chain with a memoryless property i sconstructed to draw samples from the posterior.
Although we focus our efforts on estimating the patterns of variation in codon substitution models our implementation is generally applicable to any parameters that exhibit across-data or across-time heterogeneity .
Below we describe the main aspects of the model.

\section{Methods}

We use Dirichlet process prior (DPP) to model branch specific codon parameters.
In real-world applications not every branch in the tree would display the same type of evolutonary behaviour, therefore it would be natural to expect some degree of clustering between them.
Suppose one is interested in a branch-specific parameter $\theta_{i}\;i=1,\ldots,N$, where $N=2n-1$ is the number of branches in a tree with $n$ taxa. 
This parameter might be multi-dimensional, yet for simplicity our notation will be limited to the univariate case.
We expect \emph{a priori} that $K\ll N$, where $K$ denotes the number of unique clusters of branches.
All of the branch-specific parameters have the same distribution $B$:

\begin{equation}
\forall i=1,\ldots,N \; \theta_{i}\sim B(\mu_{z_{i}}),
\label{eq:dpp1}
\end{equation}

\noindent
Where we will use $\mathbf{z}$ to denote the vector of branch category assignments.
Each branch receives a category $z_{i},\; i\in\left\{ 1,\ldots K\right\}$, where $K\in\left\{ 1,\ldots,N\right\}$ denotes the a number of branch categories.
As an example let us consider a topology $\mathbf{F}$ with $N=3$ branches and a fixed number of categories $K=2$.
One possible realization of the vector of assignments would then be:

$$\mathbf{z}=\left(z(1)=1,\; z(2)=1,\; z(3)=2\right),$$ 

\noindent
meaning that branches one and two belong to the same category 1, and that the third branch belongs to the category 2.
Let us denote the uniquely realized branch-specific parameter values by $\hat{\theta_{j}}\; j=1,\ldots,K$.
Mechanistic function $f$ keeps track of the mapping between the branches and unique realizations $\hat{\theta_{j}}$:

\begin{equation}
\mu_{z_{1}},\ldots,\mu_{z_{N}}=f\left(\hat{\theta}_{1},\ldots,\hat{\theta}_{K},z_{1},\ldots z_{N}\right).
\label{eq:dpp2}
\end{equation}

\noindent
The idea behind using DP priors is to set up a kernel distribution with probability distribution function $P_{0}$ from which unique branch-specific candidate values $\hat{\theta_{j}}$ are drawn:

\begin{equation}
\forall i=1,\ldots,N\;\hat{\theta_{j}}\sim P_{0}\left(\mu\right).
\label{eq:dpp3}
\end{equation}

\noindent
Clustering indicators for each branch are sampled according to a Dirichlet process with intensity $\gamma$:

\begin{equation}
z_{1},\ldots,z_{N}\sim DP(\gamma).
\label{eq:dpp4}
\end{equation}

\noindent
By defining hyper-priors for the parameters of kernel distribution and concentration parameter of the DP, or alternatively fixing them to a specific values, we complete the specification of the DPP:

\begin{equation}
\begin{array}{c}
\mu\sim P_{1}(\ldots)\\
\gamma\sim P_{2}(\ldots)
\end{array}.
\label{eq:dpp5}
\end{equation}

% One of them is to consider the branch-specific values as coming from a mixture of distributions
There are several possible implementations of the Dirichlet process for drawing the cluster indicators as defined in Equation \ref{eq:dpp4}. 
One of them is the so called "stick-breaking" construction \citep{Sethuraman94}.
The draws from a DP are composed of a weighted sum of point masses $P$, summing up to 1 and giving rise to a discrete distribution.
The algorithm to generate a single draw from DP is given in Listing \ref{alg:stickBreaking}.

\begin{algorithm}[H]
\begin{center}
\begin{algorithmic}[1]
% \footnotesize{
%
\State $remainingLength \gets 1.0$;
%
\For{$\left( \text{int } j=0; \; j<K; \; j++\right)$}
%
\State $r\sim Beta\left(1,\gamma\right)$;
%
\State $P\left[j\right]=r \cdot remainingLength$;
%
\State $remainingLength=\left(1-r\right) \cdot remainingLength$;
%
\EndFor \\
%
 \textbf{return} $P$;
% }
\end{algorithmic}
\end{center}
\caption{ 
{ \footnotesize 
{\bf Constructing the Dirichlet process by stick breaking.} 
}% END: footnotesize
}
\label{alg:stickBreaking}
\end{algorithm}

Colloquially we can describe it as follows: we start with a stick of length 1 and break it randomly at point $r_{1}$ chosen by drawing one value from $\text{Beta}(1, \gamma)$ and assign $p_{1}$ to the length of the part of the stick that we just broke off.
We then recursively break other portions of the stick to obtain $p_{2}, p_{3}, \ldots$ and so forth, each time setting:

\begin{equation}
p_{i}=r_{i}\cdot\underset{j}{\overset{i-1}{\prod}}\left(1-r_{j}\right).
\label{eq:sticks}
\end{equation}

\noindent
Parameter $\gamma$ controls the clustering behaviour of the process.
Smaller values will lead to a fewer, yet more populated categories, large values will result in more categories being occupied by less branches.

Under our DPP model both the number of categories $K$ and category assignments $z_{i}$ are random variables, controlled by a DPP with a concentration parameter $\gamma$ and a base distribution given by $P_{0}$.
The complete likelihood of the model can be written down as:

\begin{equation}
L(\mathbf{z},K|\gamma,N,\hat{\theta_{1}},\ldots,\hat{\theta_{K}})=\gamma^{K}\cdot\frac{\underset{j=1}{\overset{K}{\prod}}\left(\eta_{j}-1\right)!}{\underset{i=1}{\overset{N}{\prod}}\left(\gamma+i-1\right)}\cdot\underset{j=1}{\overset{K}{\prod}}P_{0}\left(\hat{\theta_{j}}\right)^{\eta_{j}},
\label{eq:dppLike} 
\end{equation} 

\noindent 
where $\eta_{k}$ denotes the number of sites assigned to category $k$.


% \section{Results}
% TODO: galaxy data
% bugs code



% \section{Discussion}


